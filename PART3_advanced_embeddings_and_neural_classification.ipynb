{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PART 3: Advanced word and document embeddings and neural classification.\n",
    "\n",
    "Here we only use Flair package, which is probably the most easy-to-use Python library to use state-of-the art pretrained neural language models. Using pretrained models gives major advantage as it allows leveraging information from huge datasets to smaller NLP problems (your problem). We only need to fine-tune a pretrained model to perform well for a new problem.\n",
    "\n",
    "We'll take preprocessed data from PART 1, which we load here from pickle file.\n",
    "\n",
    "For full documents and examples, see\n",
    "https://github.com/zalandoresearch/flair/tree/master/resources/docs\n",
    "https://www.analyticsvidhya.com/blog/2019/02/flair-nlp-library-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root folder of data with pickle file\"\n",
    "DATA_ROOT = r'D:\\Downloads\\NLP_introduction-20191023T064334Z-001\\NLP_introduction' + r'\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with simple word vectors which are included also in Flair (Fasttext type). These embeddings are not context-dependent but always fixed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple word embedding for \"Token: 1 hiilijalanjälki\": [ 0.28942   -0.25729    0.2793    -0.14168    0.21692    0.33281\n",
      " -0.076142   0.21425   -0.14987    0.50143    0.38442   -0.030661\n",
      " -0.54667   -0.06934    0.1375    -0.40508    0.58006   -0.10255\n",
      "  0.13013    0.20505    0.0084429  0.18679   -0.17214   -0.015876\n",
      "  0.16071    0.012619  -0.26008    0.5827     0.13461    0.38794\n",
      "  0.27849    0.31263   -0.28229   -0.29986   -0.36067    0.57393\n",
      "  0.45562    0.25721   -0.16588   -0.34081   -0.029271  -0.053188\n",
      " -0.28379   -0.31579   -0.16162   -0.044539   0.11141   -0.56292\n",
      "  0.042089  -0.17313   -0.10631   -0.046749  -0.37972    0.12351\n",
      " -0.14223   -0.55344   -0.4255     0.21749    0.56593   -0.30287\n",
      " -0.4045     0.28351    0.14293   -0.15708    0.56132    0.8697\n",
      " -0.48887    0.1861     0.092133  -0.0092559  0.50473   -0.090265\n",
      " -0.60152    0.0038192 -0.12302   -0.11521   -0.25384    0.27161\n",
      " -0.18558   -0.12193    0.44237   -0.017731   0.10056   -0.10506\n",
      "  0.33185    0.029694   0.25036   -0.01475   -0.046522  -0.16459\n",
      "  0.3026    -0.10684   -0.2991    -0.27709    0.25326   -0.55546\n",
      " -0.15876    0.29962   -0.10527   -0.05586    0.25361    0.23304\n",
      "  0.019902  -0.13194    0.022556   0.25256    0.20529   -0.011863\n",
      " -0.031525   0.53521    0.13098   -0.22209   -0.57328    0.36489\n",
      "  0.10243   -0.20215    0.30593   -0.196     -0.11562   -0.057389\n",
      "  0.11936    0.25865   -0.19944   -0.081679   0.14382    0.12074\n",
      "  0.30138   -0.25047    0.14298    0.31864    0.41142   -0.47238\n",
      "  0.38902   -0.36082   -0.082178  -0.26895   -0.11036    0.2568\n",
      "  0.31765    0.50512   -0.28923    0.17455    0.068033  -0.42855\n",
      "  0.11688    0.34475    0.20816   -0.30753    0.44415    0.28467\n",
      " -0.11962   -0.074894  -0.13262   -0.27355    0.3295     0.097388\n",
      "  0.11034    0.57678    0.011773   0.20002    0.58954    0.011203\n",
      " -0.026921   0.026805   0.21749   -0.063212   0.15702   -0.34077\n",
      " -0.034664   0.19532   -0.027008  -0.22731    0.1837     0.049019\n",
      " -0.45487   -0.017124  -0.5462    -0.32459   -0.17495   -0.36811\n",
      " -0.031007   0.17944    1.0173    -0.063203   0.071194   0.16735\n",
      " -0.061783   0.22475   -0.085627   0.21557    0.0095471  0.025762\n",
      "  0.082467  -0.025977   0.018889   0.078573   0.1789    -0.10124\n",
      " -0.12145   -0.26368   -0.26963    0.3355     0.12678   -0.5474\n",
      "  0.42602    0.35639   -0.13976   -0.36289   -0.011346   0.068288\n",
      " -0.08444   -0.76956   -0.084639   0.13038    0.0075209 -0.11638\n",
      " -0.36114   -0.27019   -0.53722    0.52102   -0.38413   -0.43742\n",
      " -0.094096  -0.37896    0.26981   -0.15658   -0.48709   -0.1466\n",
      " -0.079163  -0.037735  -0.058916  -0.37373   -0.24362   -0.051032\n",
      "  0.16443   -0.29934   -0.021806   0.35721   -0.0074965 -0.060562\n",
      "  0.42204   -0.11852   -0.25117    0.38696    0.1653    -0.39924\n",
      "  0.35362    0.25698   -0.05579    0.069597  -0.18712   -0.27531\n",
      "  0.060658   0.19459   -0.046958   0.19161    0.088081   0.1248\n",
      " -0.063392  -0.11042    0.28177    0.36128    0.53888    0.27671\n",
      "  0.41126    0.55926    0.11971   -0.40139   -0.80034   -0.053775\n",
      "  0.41137   -0.10764   -0.012225  -0.071518   0.27785   -0.20598\n",
      " -0.030054  -0.30758    0.16718    0.31849   -0.2545     0.13652\n",
      " -0.26626   -0.18412   -0.11023    0.02657    0.0031852 -0.22335\n",
      "  0.30067   -0.31273    0.31655   -0.023929  -0.14694   -0.51334\n",
      " -0.14765    0.043882  -0.25392    0.2991    -0.086587  -0.25087  ]\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import WordEmbeddings\n",
    "word_embedding = WordEmbeddings('fi') # fasttext word embeddings for Finnish\n",
    "\n",
    "from flair.data import Sentence\n",
    "sentence = Sentence('hiilijalanjälki')\n",
    "word_embedding.embed(sentence)\n",
    "# now check out the embedded tokens.\n",
    "vector = sentence[0].embedding.cpu().detach().numpy() # result is tensor, we want convert it to normal vector\n",
    "print('Simple word embedding for \"%s\": %s' % (sentence[0],str(vector)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we get embeddings which depend on context. These embeddings come from models, not just from a big fixed table like simple embeddings. In Flair, we can easily combine multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contextual embedding for \"Token: 2 hiilijalanjälki\": [ 1.5275617e-04 -1.0290725e-01  4.7491589e-03 ...  5.4320967e-01\n",
      "  3.1850666e-01 -3.9369926e-01]\n",
      "Contextual embedding for \"Token: 2 hiilijalanjälki\": [ 1.8941310e-04 -6.0013626e-02  3.3751559e-03 ...  2.9572883e-01\n",
      "  1.4452136e-01 -4.6072593e-01]\n",
      "Correlation between vectors 0.978912\n"
     ]
    }
   ],
   "source": [
    "from flair.embeddings import FlairEmbeddings,WordEmbeddings,BertEmbeddings\n",
    "\n",
    "# https://github.com/stefan-it/flair-lms#multilingual-flair-embeddings\n",
    "\n",
    "# init Flair embeddings\n",
    "flair_forward_embedding = FlairEmbeddings('fi-forward')\n",
    "flair_backward_embedding = FlairEmbeddings('fi-backward')\n",
    "\n",
    "# init multilingual BERT\n",
    "bert_embedding = BertEmbeddings('bert-base-multilingual-cased')\n",
    "\n",
    "from flair.embeddings import StackedEmbeddings\n",
    "\n",
    "# now create the StackedEmbedding object that combines all embeddings\n",
    "stacked_embeddings = StackedEmbeddings(\n",
    "    embeddings=[flair_forward_embedding, flair_backward_embedding, bert_embedding])\n",
    "\n",
    "# Next we test the embeddings for example sentences\n",
    "from flair.data import Sentence\n",
    "import numpy as np\n",
    "\n",
    "# make two highly similar sentences\n",
    "sentence=[None]*2\n",
    "sentence[0] = Sentence('Suomalaisen hiilijalanjälki on vuodessa keskimäärin noin 11 tonnia hiilidioksidiksi muutettuna .')\n",
    "sentence[1] = Sentence('Japanilaisen hiilijalanjälki on vuodessa enintään noin 11 tonnia hiilidioksidiksi muutettuna .')\n",
    "# NOTE: All tokens should be separated by space, no \"smart\" tokenizer is used here!\n",
    "\n",
    "# get embedding for the second word, which was same in both sentences\n",
    "vectors=[None]*2\n",
    "for i,s in enumerate(sentence):\n",
    "    # just embed a sentence using the StackedEmbedding as you would with any single embedding.\n",
    "    stacked_embeddings.embed(s)\n",
    "    # now check out the embedded tokens.\n",
    "    token=s[1]\n",
    "    vectors[i] = token.embedding.cpu().detach().numpy()\n",
    "    print('Contextual embedding for \"%s\": %s' % (token,str(vectors[i])))\n",
    "print(\"Correlation between vectors %f\" % (np.corrcoef(vectors)[0,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we take above idea a step further: We fine-tune above models to create document embeddings that are used with a classifier. This requires some ~10 mins using PyTorch and GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flair.data import Corpus\n",
    "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n",
    "from flair.models import TextClassifier\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.datasets import CSVClassificationCorpus\n",
    "\n",
    "# Step 1: Create .CSV files of our data\n",
    "import pickle\n",
    "import random\n",
    "import pandas\n",
    "import csv\n",
    "\n",
    "data=pickle.load(open(DATA_ROOT + 'turkuNLP_preprocessed_data.pickle','rb'))\n",
    "# create shuffled indices\n",
    "ind = list(range(len(data)))\n",
    "random.seed(1)\n",
    "random.shuffle(ind)\n",
    "\n",
    "# create training, development and testing splits using ratios 8:1:1\n",
    "# save each split into own file\n",
    "MAX_TOKENS = 1000 # need to limit document size or else run out of memory :(\n",
    "cutter = lambda x:x[1:] if len(x)<MAX_TOKENS else x[1:MAX_TOKENS] # first token is just repeat, skip...\n",
    "low = 0\n",
    "for frac,file in zip([0.8,0.9,1.0],['train.csv','dev.csv','test.csv']):\n",
    "    up = round(frac*len(data))\n",
    "    frame = pandas.DataFrame()\n",
    "    frame['label'] = [data[ind[i]]['label'] for i in range(low,up)]\n",
    "    frame['text'] = [\" \".join(cutter(data[ind[i]]['tokens_raw'])) for i in range(low,up)]\n",
    "    # use pandas for easy file saving\n",
    "    frame.to_csv(DATA_ROOT + file,encoding=\"utf-8\",sep=\"\\t\",index=False,quoting=csv.QUOTE_NONE)\n",
    "    low = up\n",
    "\n",
    "# this is the folder in which train, test and dev files reside\n",
    "data_folder = DATA_ROOT\n",
    "\n",
    "# column format indicating which columns hold the text and label(s)\n",
    "column_name_map = {0: \"label_topic\",1: \"text\"} # note: 0 = first column!\n",
    "\n",
    "# load corpus containing training, test and dev data and if CSV has a header, you can skip it\n",
    "corpus: Corpus = CSVClassificationCorpus(data_folder,\n",
    "                                         column_name_map,\n",
    "                                         skip_header=True,\n",
    "                                         delimiter='\\t',    # tab-separated files\n",
    "                                         in_memory=True\n",
    ")\n",
    "# 2. create the label dictionary\n",
    "label_dict = corpus.make_label_dictionary()\n",
    "\n",
    "# 3. make a list of word embeddings\n",
    "word_embeddings = [WordEmbeddings('fi'),\n",
    "                   #FlairEmbeddings('fi-forward'), # can add, if enough memory\n",
    "                   #FlairEmbeddings('fi-backward'),\n",
    "                   ]\n",
    "\n",
    "# 4. initialize document embedding by passing list of word embeddings\n",
    "# Can choose between many RNN types (GRU by default, to change use rnn_type parameter)\n",
    "document_embeddings: DocumentRNNEmbeddings = DocumentRNNEmbeddings(word_embeddings,\n",
    "                                                                     hidden_size=512,\n",
    "                                                                     reproject_words=True,\n",
    "                                                                     reproject_words_dimension=150,\n",
    "                                                                     )\n",
    "\n",
    "# 5. create the text classifier\n",
    "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\n",
    "\n",
    "# 6. initialize the text classifier trainer\n",
    "trainer = ModelTrainer(classifier, corpus)\n",
    "\n",
    "# 7. start the training\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "trainer.train(data_folder+\"flair_temp\", # Main path to which all output during training is logged and models are saved\n",
    "              learning_rate=0.1,\n",
    "              mini_batch_size=40,\n",
    "              anneal_factor=0.5,\n",
    "              patience=4,\n",
    "              max_epochs=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, we can load the model and use it to make predictions for new texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally we test the model with couple of text segments\n",
    "classifier = TextClassifier.load(data_folder+r\"flair_temp\\final-model.pt\")\n",
    "\n",
    "# create and test example sentences\n",
    "from flair.data import Sentence\n",
    "sentence = Sentence('Jälleenhankintahinnalla tarkoitetaan omaisuuden uushankintahintaa . Tällöin voidaan viitata siihen , mihin hintaan esimerkiksi tietty kone tai laite voitaisiin korvata hankkimalla se markkinoilta tänä päivänä .')\n",
    "classifier.predict(sentence)\n",
    "print('Predicted class for text \"%s\"\\n%s' % (sentence.to_plain_string(),sentence.labels))\n",
    "\n",
    "sentence = Sentence('Elinluovutuskortti antaa luvan käyttää kortin täyttäneen henkilön elimiä ja kudoksia kuoleman jälkeen toisten henkilöiden hengen pelastamiseksi tai terveyden parantamiseksi . Vuoden 2010 lakimuutoksen jälkeen kortti ei enää ole välttämätön , sillä nykyisin oletetaan vainajan suostuneen elintensä luovutukseen , ellei hänen tiedetä sitä elinaikanaan erityisesti kieltäneen . Elinluovutuskortin mukana kantaminen on edelleen hyvä varmistaa tahtonsa toteutuminen . Suomalaisista noin 18 prosenttia on allekirjoittanut elinluovutuskortin .')\n",
    "classifier.predict(sentence)\n",
    "print('Predicted class for text \"%s\"\\n%s' % (sentence.to_plain_string(),sentence.labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
