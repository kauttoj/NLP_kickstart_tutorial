{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#PART 1: Text preprocessing and classic TF-IDF based classification\n",
    "\n",
    "We use following packages:\n",
    "-NLTK\n",
    "-SpaCy\n",
    "-Voikko\n",
    "-Polyglot\n",
    "-TurkuNLP neural parser\n",
    "-Scikit-learn\n",
    "\n",
    "We use toy dataset \"wikipedia_toydata_FIN\" with 416 finnish wikipedia articles related to health (212 texts) and economy (204 texts).\n",
    "There are two versions of the data:\n",
    " Standard version \"wikipedia_toydata_FIN_simple.txt\"\n",
    " Version suitable for neural parser only \"wikipedia_toydata_FIN_commented.txt\"\n",
    "\n",
    "Material and tutorials:\n",
    "https://github.com/TurkuNLP/Text_Mining_Course/blob/master/Elementary%20text%20processing.ipynb\n  ",
    "https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n  ",
    "https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n  ",
    "https://data.solita.fi/finnish-stemming-and-lemmatization-in-python/\n  ",
    "https://polyglot.readthedocs.io/en/latest/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root folder of data\n",
    "DATA_ROOT = r'D:\\Downloads\\NLP_introduction-20191023T064334Z-001\\NLP_introduction' + r'\\\\'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typical steps for processing text are:\n",
    "1. Tokenizing (separate words)\n",
    "2. Spellchecking/correcting\n",
    "3. Part of speech (POS) tagging\n",
    "4. Lemmatizing (getting basewords)\n",
    "5. Named entity recognition (NER tagging)\n",
    "\n",
    "The specific order of steps 2-5 is somewhat \"gray zone\" and depends on which tool/pipeline is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing with NLTK\n",
      "processing with SpaCy\n",
      "processing with Polyglot\n",
      "processing with Voikko\n",
      "..proofreading and lemmatizing\n",
      "all done\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Simple text preprocessing using NLTK, SpaCy, Polyglot and Voikko.\n",
    "\n",
    "We create a list of text samples where each element is a dictionary with fields:\n",
    " label: either TERVEYS or TALOUS\n",
    " tokens_raw: list of raw tokens\n",
    " tokens_lemma: list of lemmatized tokens\n",
    " tokens_type: list of token types\n",
    " tokens_ner: list of token named entity (empty if none)\n",
    "\n",
    "NOTE: NLTK and SpaCy do not directly support Finnish, hence they are typically only useful for tokenization.\n",
    "\n",
    "'''\n",
    "import pandas\n",
    "\n",
    "# read labels and texts\n",
    "A = pandas.read_csv(DATA_ROOT + r'wikipedia_toydata_FIN_simple.txt', delimiter=\"\\t\", encoding=\"utf-8\", names=['label', 'text'])\n",
    "labels = list(A.label) # labels\n",
    "samples = list(A.text) # texts\n",
    "\n",
    "# use NLTK\n",
    "from nltk.tokenize import word_tokenize\n",
    "print('processing with NLTK')\n",
    "data_nltk = [{'label':labels[k],'tokens_raw':word_tokenize(x)} for k,x in enumerate(samples)] # only one model\n",
    "# NOTE: No lemmatization or other features for Finnish\n",
    "\n",
    "# use SpaCy\n",
    "import spacy\n",
    "# You need to download \"xx_ent_wiki_sm\" with \"python -m spacy download xx_ent_wiki_sm\"\n",
    "nlp = spacy.load(\"xx_ent_wiki_sm\") # load multilingual model, no model for Finnish\n",
    "print('processing with SpaCy')\n",
    "data_spacy= [{'label':labels[k],\n",
    "            'tokens_raw':[x.text for x in nlp(sample)],\n",
    "            'tokens_lemma':[x.lemma_ for x in nlp(sample)],\n",
    "            'tokens_pos':[x.pos_ for x in nlp(sample)]} for k,sample in enumerate(samples)\n",
    "            ]\n",
    "# NOTE: Lemmas are just raw tokens and pos are empty, not useful for Finnish\n",
    "\n",
    "# use Polyglot\n",
    "from polyglot.text import Text\n",
    "print('processing with Polyglot')\n",
    "data_polyglot=[]\n",
    "for k,sample in enumerate(samples):\n",
    "    text = Text(sample,hint_language_code='fi')\n",
    "    # populate tags and entities\n",
    "    text.pos_tags\n",
    "    text.entities\n",
    "    # get tokens\n",
    "    tokens_raw,tokens_pos = zip(*[[x[0],x[1]] for x in text.pos_tags])\n",
    "    # initialize entities (empty if none)\n",
    "    tokens_ner = ['' for x in tokens_raw]     \n",
    "    # populate found entities\n",
    "    for ent in text.entities:\n",
    "        start=ent.start\n",
    "        stop =ent.end\n",
    "        for i in range(start,stop):\n",
    "            tokens_ner[i] = ent.tag \n",
    "    # add sample\n",
    "    data_polyglot.append({'label':labels[k],'tokens_raw':tokens_raw,'tokens_pos':tokens_pos,'tokens_ner':tokens_ner})\n",
    "# NOTE: No lemmatization available\n",
    "\n",
    "# use Voikko\n",
    "from voikko import libvoikko\n",
    "voikko_object = libvoikko.Voikko(u\"fi\") # Voikko object for Finnish\n",
    "print('processing with Voikko')\n",
    "data_voikko=[{'label':labels[k],'tokens_raw':[x.tokenText for x in voikko_object.tokens(sample) if x.tokenTypeName != 'WHITESPACE']} for k,sample in enumerate(samples)]\n",
    "\n",
    "print('..proofreading and lemmatizing')\n",
    "for sample in data_voikko:\n",
    "    lemmas=[]\n",
    "    poss = []\n",
    "    for k,token_raw in enumerate(sample['tokens_raw']):\n",
    "        suggested_words = voikko_object.suggest(token_raw) # get suggestions\n",
    "        if len(suggested_words)>0:\n",
    "            token_raw = suggested_words[0] # replace raw token with first suggestion\n",
    "            sample['tokens_raw'][k] = token_raw # update with new word\n",
    "\n",
    "        # Analyze the word with voikko\n",
    "        voikko_dict = voikko_object.analyze(token_raw)\n",
    "        # Extract the base form, if the word is recognized\n",
    "        if voikko_dict:\n",
    "            token_lemma = voikko_dict[0]['BASEFORM']\n",
    "            pos = voikko_dict[0]['CLASS']\n",
    "        # If word is not recognized, add the original word\n",
    "        else:\n",
    "            token_lemma = token_raw\n",
    "            pos = ''\n",
    "        lemmas.append(token_lemma)\n",
    "        poss.append(pos)\n",
    "    sample['tokens_lemma']=lemmas\n",
    "    sample['tokens_pos']=poss\n",
    "\n",
    "print('all done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll use advanced all-in-one preprocessing pipeline \"Turku NLP neural parser\". For best results ideally all steps should be done simultaneously as there are dependencies, especially in finding lemmas. This is recommended method, especially for Finnish!\n",
    "\n",
    "Running neural parser is bit tricky and is done outside Python. Software is distributed as a Docker image.\n",
    "\n",
    "A. First install Docker, which is available for free for Win, Mac and Linux.\n",
    "\n",
    "B. Start console/terminal with docker available and start processing using these commands\n",
    "\n",
    "In Linux:  \n",
    " 'cat wikipedia_toydata_FIN_commented.txt | docker run -i turkunlp/turku-neural-parser:latest-fi-en-sv-cpu stream fi_tdt parse_plaintext > wikipedia_toydata_FIN_commented_parsed.txt'  \n",
    "\n",
    "In Windows PowerShell (2 commands):  \n",
    " '$OutputEncoding = [Console]::OutputEncoding = [Text.UTF8Encoding]::UTF8'  \n",
    " 'Get-Content -Encoding UTF8 wikipedia_toydata_FIN_commented.txt | docker run -i turkunlp/turku-neural-parser:latest-fi-en-sv-cpu stream fi_tdt parse_plaintext > wikipedia_toydata_FIN_commented_parsed.txt'  \n",
    "\n",
    "First run takes longer as the image is first downloaded and installed into system. After that, it's faster.\n",
    "\n",
    "C. After finished parsing, result is in CONLL-U format which needs to be parsed.\n",
    "Example output (first lines) looks like this:\n",
    "\n",
    "\\# newdoc  \n",
    "\\# newpar  \n",
    "\\# sent_id = 1  \n",
    "\\# text = ﻿Dummy  \n",
    "1\t﻿Dummy\t﻿Dummy\tVERB\tSymb\t_\t0\troot\t_\tSpacesAfter=\\r\\n\n",
    "\n",
    "\\# sample001  \n",
    "\\# newdoc  \n",
    "\\# newpar  \n",
    "\\# sent_id = 1  \n",
    "\\# text = TERVEYS Kuuleva Kuuleva on henkilö, jonka molemmissa korvissa on normaali kuulo tai jonka kuulonalenema on niin lievä, ettei se haittaa hänen jokapäiväistä elämäänsä.  \n",
    "1\tTERVEYS\tterveys\tNOUN\tN\tCase=Nom|Derivation=Vs|Number=Sing\t2\tobj\t_\tSpacesAfter=\\t\\s  \n",
    "2\tKuuleva\tkuulla\tVERB\tV\tCase=Nom|Degree=Pos|Number=Sing|PartForm=Pres|VerbForm=Part|Voice=Act\t3\tacl\t_\t_  \n",
    "3\tKuuleva\tkuuleva\tNOUN\tA\tCase=Nom|Degree=Pos|Number=Sing\t5\tnsubj:cop\t_\t_  \n",
    "4\ton\tolla\tAUX\tV\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t5\tcop\t_\t_  \n",
    "5\thenkilö\thenkilö\tNOUN\tN\tCase=Nom|Number=Sing\t0\troot\t_\tSpaceAfter=No  \n",
    "6\t,\t,\tPUNCT\tPunct\t_\t9\tpunct\t_\t_  \n",
    "7\tjonka\tjoka\tPRON\tPron\tCase=Gen|Number=Sing|PronType=Rel\t9\tnmod:poss\t_\t_  \n",
    "8\tmolemmissa\tmolemmat\tPRON\tPron\tCase=Ine|Number=Plur|PronType=Ind\t9\tdet\t_\t_  \n",
    "9\tkorvissa\tkorva\tNOUN\tN\tCase=Ine|Number=Plur\t5\tacl:relcl\t_\t_  \n",
    "10\ton\tolla\tAUX\tV\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t9\tcop\t_\t_  \n",
    "11\tnormaali\tnormaali\tADJ\tA\tCase=Nom|Degree=Pos|Number=Sing\t12\tamod\t_\t_  \n",
    "12\tkuulo\tkuulo\tNOUN\tN\tCase=Nom|Number=Sing\t9\tnsubj:cop\t_\t_  \n",
    "13\ttai\ttai\tCCONJ\tC\t_\t18\tcc\t_\t_  \n",
    "14\tjonka\tjoka\tPRON\tPron\tCase=Gen|Number=Sing|PronType=Rel\t15\tnmod:poss\t_\t_  \n",
    "15\tkuulonalenema\tkuulon#alenema\tNOUN\tN\tCase=Nom|Number=Sing\t18\tnsubj:cop\t_\t_  \n",
    "16\ton\tolla\tAUX\tV\tMood=Ind|Number=Sing|Person=3|Tense=Pres|VerbForm=Fin|Voice=Act\t18\tcop\t_\t_  \n",
    "17\tniin\tniin\tADV\tAdv\t_\t18\tadvmod\t_\t_  \n",
    "18\tlievä\tlievä\tADJ\tA\tCase=Nom|Degree=Pos|Number=Sing\t9\tconj\t_\tSpaceAfter=No  \n",
    "19\t,\t,\tPUNCT\tPunct\t_\t22\tpunct\t_\t_  \n",
    "20\tettei\tettä#ei\tVERB\tV\tNumber=Sing|Person=3|Polarity=Neg|VerbForm=Fin|Voice=Act\t22\tmark\t_\t_  \n",
    "21\tse\tse\tPRON\tPron\tCase=Nom|Number=Sing|PronType=Dem\t22\tnsubj\t_\t_  \n",
    "22\thaittaa\thaitata\tVERB\tV\tConnegative=Yes|Mood=Ind|Tense=Pres|VerbForm=Fin\t18\tccomp\t_\t_  \n",
    "23\thänen\thän\tPRON\tPron\tCase=Gen|Number=Sing|Person=3|PronType=Prs\t25\tnmod:poss\t_\t_  \n",
    "24\tjokapäiväistä\tjokapäiväinen\tADJ\tA\tCase=Par|Degree=Pos|Derivation=Inen|Number=Sing\t25\tamod\t_\t_  \n",
    "25\telämäänsä\telämä\tNOUN\tN\tCase=Par|Number=Sing|Person[psor]=3\t22\tobj\t_\tSpaceAfter=No  \n",
    "26\t.\t.\tPUNCT\tPunct\t_\t5\tpunct\t_\t_  \n",
    "\n",
    "\\# sent_id = 2\n",
    "\n",
    "For illustrated example, see:  \n",
    "'http://bionlp-www.utu.fi/parser_demo'\n",
    "\n",
    "We'll use conllu package and custom code to parse the data. Here \"# sample001\" are comments that are needed to separate different samples. For the same reason, we needed to add dummy text at the beginning.\n",
    "\n",
    "NOTE: Output file is in UTF-16 format (for some reason)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing textfile\n",
      "Done. Total 416 samples after parsing (labels ['TALOUS' 'TERVEYS'])\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Parsing CONLL-U format file coming from Turku NLP neural parser with assumed file name \"wikipedia_toydata_FIN_commented_parsed.txt\"\n",
    "'''\n",
    "\n",
    "from conllu import parse # parser\n",
    "import re\n",
    "\n",
    "sample_separator = re.compile('# sample...') # separator of samples\n",
    "data_turkuNLP = [] # collect samples here\n",
    "\n",
    "# parse one sample\n",
    "def get_sample(content,data):\n",
    "    if len(content)==0:\n",
    "        return\n",
    "    sentences = parse(content)\n",
    "    tokens_raw = []\n",
    "    tokens_lemma = []\n",
    "    tokens_pos = []\n",
    "    label = None\n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            if label == None:\n",
    "                label = token['form']\n",
    "            else:\n",
    "                tokens_raw.append(token['form'])\n",
    "                tokens_lemma.append(token['lemma'])\n",
    "                tokens_pos.append(token['upostag'])\n",
    "    data_turkuNLP.append({'label': label, 'tokens_raw': tokens_raw, 'tokens_lemma': tokens_lemma,'tokens_pos':tokens_pos})\n",
    "\n",
    "# read parsed dataset and make samples\n",
    "print('parsing textfile')\n",
    "with open(DATA_ROOT + 'wikipedia_toydata_FIN_commented_parsed.txt','r',encoding=\"utf-16\") as f:\n",
    "    is_reading=False\n",
    "    content = ''\n",
    "    for line_num, line in enumerate(f):\n",
    "        if len(sample_separator.findall(line))>0: # sample separator present\n",
    "            get_sample(content,data_turkuNLP)\n",
    "            content = ''\n",
    "            is_reading = True\n",
    "        elif is_reading:\n",
    "            content += line\n",
    "    get_sample(content,data_turkuNLP)\n",
    "\n",
    "import numpy as np\n",
    "print('Done. Total %i samples after parsing (labels %s)' % (len(data_turkuNLP),np.unique([x['label'] for x in data_turkuNLP])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note on Named Entiry Regognition (NER). Apart from Polyglot, there are no Python tools to perform NER tagging in Finnish (as far as I know). Such tool is also not available from TurkuNLP group. \n",
    "\n",
    "There is one Linux binary:\n",
    "https://korp.csc.fi/download/finnish-tagtools/v1.3/\n",
    "Using this requires some effort on feeding text files and parsing the results as another text files. But in principle, it's doable and one can then add yet another field \"tokens_ner\" into above data. We'll skip this tool for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data_turkuNLP as pickle. We'll use it later.\n",
    "import pickle\n",
    "pickle.dump(data_turkuNLP,open(DATA_ROOT + 'turkuNLP_preprocessed_data.pickle','wb'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Finally we'll use our preprocessed texts to do binary topic classification with sklearn. \n",
    "For this, we'll use classic one-hot encoding of words and 2-grams followed by TF-IDF scaling. This is the classic baseline approach,\n",
    "but it's still widely used and valid particularly for small datasets. For large datasets, neural network models are superior.\n",
    "\n",
    "We'll use data_turkuNLP preprocessed data, but also simple build-in preprocessor in sklearn. \n",
    "We also try two different classifiers: Naive Bayes and NuSVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classifying\n",
      "..Preprocessor \"TurkuNLP\", data type \"tokens_raw\", classifier \"MultinomialNB\": Mean accuracy 0.944763, mean F1 0.944643\n",
      "..Preprocessor \"TurkuNLP\", data type \"tokens_raw\", classifier \"NuSVM\": Mean accuracy 0.907890, mean F1 0.907159\n",
      "..Preprocessor \"TurkuNLP\", data type \"tokens_lemma\", classifier \"MultinomialNB\": Mean accuracy 0.959223, mean F1 0.959113\n",
      "..Preprocessor \"TurkuNLP\", data type \"tokens_lemma\", classifier \"NuSVM\": Mean accuracy 0.953946, mean F1 0.953866\n",
      "..Preprocessor \"Sklearn\", data type \"tokens_raw\", classifier \"MultinomialNB\": Mean accuracy 0.934949, mean F1 0.934758\n",
      "..Preprocessor \"Sklearn\", data type \"tokens_raw\", classifier \"NuSVM\": Mean accuracy 0.929672, mean F1 0.929201\n",
      "all done\n"
     ]
    }
   ],
   "source": [
    "# topic classification with sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import StratifiedKFold,cross_validate\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.svm import NuSVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "import pandas\n",
    "\n",
    "from nltk.corpus import stopwords # NLTK contains stopwords for Finnish\n",
    "fin_stop = set(stopwords.words('finnish')) # stopwords for finnish\n",
    "\n",
    "# define custom tokenizer when bypassing build-in preprocessing, we'll also skip stopwords\n",
    "my_tokenizer = lambda x:[y.lower() for y in x if y.lower() not in fin_stop] \n",
    "\n",
    "# k-fold cross validation\n",
    "cv = StratifiedKFold(n_splits=10) # 10 folds\n",
    "\n",
    "# Note: FunctionTransformer \"hack\" is needed to convert sparse features to dense\n",
    "\n",
    "# pipeline with own custom preprocessing\n",
    "text_clf_custom = {'MultinomialNB':make_pipeline(\n",
    "    TfidfVectorizer(analyzer='word',tokenizer=my_tokenizer,ngram_range=(1,2),lowercase=False,max_features=15000),FunctionTransformer(lambda x: x.todense(), accept_sparse=True,validate=True),GaussianNB()),\n",
    "    'NuSVM':make_pipeline(\n",
    "    TfidfVectorizer(analyzer='word',tokenizer=my_tokenizer,ngram_range=(1,2),lowercase=False,max_features=15000),FunctionTransformer(lambda x: x.todense(), accept_sparse=True,validate=True),NuSVC(nu=0.5,gamma='scale'))}\n",
    "\n",
    "# pipeline with sklearn build-in preprocessing\n",
    "text_clf_simple = {'MultinomialNB':make_pipeline(\n",
    "    TfidfVectorizer(stop_words = fin_stop,ngram_range=(1,2),max_features=15000),FunctionTransformer(lambda x: x.todense(), accept_sparse=True,validate=True),GaussianNB()),\n",
    "    'NuSVM':make_pipeline(\n",
    "    TfidfVectorizer(stop_words = fin_stop,ngram_range=(1,2),max_features=15000),FunctionTransformer(lambda x: x.todense(), accept_sparse=True,validate=True),NuSVC(nu=0.5,gamma='scale'))}\n",
    "\n",
    "print('Classifying')\n",
    "for preprocessor in ['TurkuNLP','Sklearn']:\n",
    "    for k,data_type in enumerate(['tokens_raw', 'tokens_lemma']):\n",
    "        if preprocessor == 'TurkuNLP':\n",
    "            text_clf = text_clf_custom # use custom pipeline            \n",
    "            data=pickle.load(open(DATA_ROOT + 'turkuNLP_preprocessed_data.pickle','rb')) # our preprocessed data\n",
    "            X = [x[data_type] for x in data]\n",
    "            y = [x['label'] for x in data]\n",
    "        else:\n",
    "            if k==1:\n",
    "                continue # don't have lemmas, skip this iteration\n",
    "            text_clf = text_clf_simple # use standard pipeline\n",
    "            A = pandas.read_csv(DATA_ROOT + 'wikipedia_toydata_FIN_simple.txt',delimiter=\"\\t\",encoding=\"utf-8\",names = ['label','text'])\n",
    "            y = list(A.label)\n",
    "            X = list(A.text)\n",
    "        for pipe in text_clf.keys():\n",
    "            scores = cross_validate(text_clf[pipe],X=X,y=y,cv=cv,scoring=['f1_macro','accuracy'])\n",
    "            print('..Preprocessor \"%s\", data type \"%s\", classifier \"%s\": Mean accuracy %f, mean F1 %f' % (preprocessor,data_type,pipe,scores['test_accuracy'].mean(),scores['test_f1_macro'].mean()))\n",
    "print('all done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Spyder)",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
